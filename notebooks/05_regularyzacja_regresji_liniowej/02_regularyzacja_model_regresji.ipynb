{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja w modelu regresji - porównanie regresji grzbietowej i regresji Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularyzacja grzbietowa** i **Lasso** są technikami, które są wykorzystywane do budowania **oszczędnych modeli**, w rozumieniu obecności zbyt dużej liczby predyktorów. \n",
    "Przez dużą liczbę predyktorów rozumiemy:\n",
    "\n",
    "- *duża liczba predyktorów* to taka, która prowadzi do **przeuczenia modelu** (ang. *overfitting*) -- nawet tak niewielka liczba jak 10 zmiennych może prowadzić do przeuczenia,\n",
    "    \n",
    "- *duża liczba predyktorów* to taka, która może prowadzić do problemów z **wydajnością obliczeniową** -- przy obecnych możliwościach komputerów, taka sytuacja może mieć miejsce przy występowaniu milionów lub miliardów cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Techniki regularyzacyjne działają poprzez \n",
    "- karanie wielkości współczynników cech, \n",
    "- minimalizowanie błędu między przewidywanymi a rzeczywistymi obserwacjami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dlaczego karamy za wielkość współczynników?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozważmy następujący przykład celem zrozumienia wpływu złożoności modelu na wielkość współczynników.\n",
    "\n",
    "W tym celu dopasujmy krzywą regresji do krzywej sinusoidalnej (od 0° do 360°) z dodanym szumem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.array([i * np.pi/180 for i in range(0, 360, 5)])\n",
    "X = pd.DataFrame(x)\n",
    "y = np.sin(x) + np.random.normal(0, .2, len(x))\n",
    "plt.plot(x, y, '.', color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2, 16): \n",
    "    colname = 'x_%d'%i      \n",
    "    data[colname] = data['x'] ** i\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_fun  = lambda y, y_pred: sum((y_pred-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X, y, power, models_to_plot):\n",
    "    reg = make_pipeline(PolynomialFeatures(power),\n",
    "                        StandardScaler(),\n",
    "                        LinearRegression())\n",
    "    reg.fit(X, y)\n",
    "    y_pred = reg.predict(X)\n",
    "    \n",
    "    if power in models_to_plot:\n",
    "        plt.subplot(models_to_plot[power])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(x, y_pred)\n",
    "        plt.plot(x, y, '.')\n",
    "        plt.title('Plot for power: %d' % power)\n",
    "    \n",
    "    ret = [rss_fun(y, y_pred)]\n",
    "    ret.extend([reg.named_steps['linearregression'].intercept_])\n",
    "    ret.extend(reg.named_steps['linearregression'].coef_[1:])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col = ['RSS', 'Intercept'] + ['coef_x_%d' % i for i in range(1, 16)]\n",
    "ind = ['model_pow_%d' % i for i in range(1, 16)]\n",
    "coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1:231, 3:232, 6:233, 9:234, 12:235, 15:236}\n",
    "\n",
    "for i in range(1, 16):\n",
    "    coef_matrix_simple.iloc[i-1, 0:i+2] = linear_regression(X, y, power=i, models_to_plot=models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie**:\n",
    "\n",
    "- wielkość współczynników regresji rośnie eksponencjalnie wraz ze wzrostem złożoności modelu,\n",
    "- wielkość współczynnika regresji wpływa na istotność zmiennej odpowiadającej temu współczynnikowi w oszacowaniu wielkość zmiennej odpowiedzi, ale gdy wielkość współczynnik jest zbyt duża, algorytm modeluje skomplikowane relacje w celu oszacowania wyników, co często kończy się zbytnim dopasowaniem do danych,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja grzbietowa (ang. *rigde regression*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda najmniejszych kwadratów z regularyzacją $l2$, minimalizuje **funkcję kryterialną**:\n",
    "\n",
    "$$||y - Xb||^2_2 + \\alpha \\cdot ||b||^2_2,$$\n",
    "\n",
    "gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_2 = \\sqrt{\\sum_{i=1}^n a_i^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$ - siła regularyzacja, $\\alpha > 0$ \n",
    "\n",
    "* gdy $\\alpha = 0$ -- problem uprasza się do zwykłej regresji\n",
    "* gdy $\\alpha = +\\infty$ -- współczynnik są równe zeru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1\n",
    "Napisz funkcję, która dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacją Ridge z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowaną funkcję regresji dla $k$ danych wartości parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n",
    "`models_to_plot`).\n",
    "\n",
    "Następnie wyznacz ramkę danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), której wiersze dla ustalonej będą zawierały: wartość RSS oraz kolejne wartości współczynników regresji dla różnych parametrów $\\alpha$, np. lista `alpha_ridge`.\n",
    "\n",
    "Sprawdź jak zmieniają się wartości współczynników regresji z regularyzacją grzbietową wraz ze zmianą parametru $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_regression(X, y, alpha, power, models_to_plot={}):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "\n",
    "col = ['RSS','Intercept'] + ['coef_x_%d'%i for i in range(1, 16)]\n",
    "ind = ['alpha_%.2g' % alpha_ridge[i] for i in range(0, len(alpha_ridge))]\n",
    "coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n",
    "\n",
    "\n",
    "power = 15\n",
    "for i in range(10):\n",
    "    coef_matrix_ridge.iloc[i,] = ridge_regression(X, y, alpha_ridge[i], power, models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie**:\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularyzacja Lasso (ang. *Lasso regression*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO - Least Absolute Shrinkage and Selection Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda najmniejszych kwadratów z regularyzacją $l1$, minimalizuje **funkcję kryterialną**:\n",
    "\n",
    "$$||y - Xb||^2_2 + \\alpha \\cdot ||b||^2_1,$$\n",
    "\n",
    "gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_1 = \\sqrt{\\sum_{i=1}^n |a_i|}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2\n",
    "Napisz funkcję, która dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacją Lasso z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowaną funkcję regresji dla $k$ danych wartości parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n",
    "`models_to_plot`).\n",
    "\n",
    "Następnie wyznacz ramkę danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), której wiersze dla ustalonej będą zawierały: wartość RSS oraz kolejne wartości współczynników regresji dla różnych parametrów $\\alpha$, np. lista `alpha_ridge`.\n",
    "\n",
    "Sprawdź jak zmieniają się wartości współczynników regresji z regularyzacją Lasso wraz ze zmianą parametru $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_regression(X, y, alpha, power, models_to_plot={}):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lasso = [1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10]\n",
    "\n",
    "col = ['RSS','Intercept'] + ['coef_x_%d' % i for i in range(1, 16)]\n",
    "ind = ['alpha_%.2g' % alpha_lasso[i] for i in range(0, 10)]\n",
    "coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n",
    "\n",
    "#Define the models to plot\n",
    "models_to_plot = {1e-10:231, 1e-5:232,1e-4:233, 1e-3:234, 1e-2:235, 1:236}\n",
    "\n",
    "#Iterate over the 10 alpha values:\n",
    "power\n",
    "for i in range(10):\n",
    "    coef_matrix_lasso.iloc[i,] = lasso_regression(X, y, alpha_lasso[i], power, models_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2g}'.format\n",
    "coef_matrix_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie**:\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porównanie regularyzacji grzbietowej z regularyzacją Lasso\n",
    "\n",
    "### Ridge\n",
    "- zawiera wszystkie (lub żadne) cechy w modelu, główną zaletą tej regularyzacji jest **ściągniecie współczynników** (ang. **shrinkage coefficient**),\n",
    "- regresji grzbietowej używa się głowniej do **uniknięcia przeuczenia** modelu, ale z racji, że zawiera wszystkie zmienne z modelu nie jest użyteczny w przypadku wielowymiarowych danych (gdy liczbę predyktorów szacuje się milionach/miliardach -- zbyt duża złożoność obliczeniowa),\n",
    "- zasadniczo działa dobrze nawet w obecności silnie **skorelowanych** cech -- uwzględnia wszystkie skorelowane zmienne w modelu, ale wielkość współczynników zależy od wielkości korelacji.\n",
    "\n",
    "\n",
    "### Lasso \n",
    "- regularyzacja Lasso poza **ściągniecie współczynników**, dokonuje również selekcji zmiennych\n",
    "- regularyzacje Lasso często wykorzystuje się do **selekcji zmiennych** w przypadku do liczba cech jest rzędu milionów/miliardów\n",
    "- wybiera dowolną cechę spośród cech silnie skorelowanych, współczynniki pozostałych cechy skorelowanych z wybraną zmienną redukuje do zera, ale wybrana zmienna zmienia się losowo wraz ze zmianą parametrów modelu -- podejście te działa gorzej niż regularyzacja grzbietowa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
